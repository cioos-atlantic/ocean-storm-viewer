{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install psycopg2 module to support SQL Alchemy and postgresql\n",
    "\n",
    "\"\"\"\n",
    "SQL Queries to be run after ingestion of data to define point geometry objects \n",
    "based on lat/lon of data for use with Geoserver, helps Geoserver define \n",
    "bounding boxes and such\n",
    "\n",
    "ALTER TABLE public.active_storms ADD COLUMN geom geometry(Point, 4326);\n",
    "ALTER TABLE public.historical_storms ADD COLUMN geom geometry(Point, 4326);\n",
    "\n",
    "UPDATE public.active_storms SET geom = ST_SetSRID(ST_MakePoint(\"LON\", \"LAT\"), 4326);\n",
    "UPDATE public.historical_storms SET geom = ST_SetSRID(ST_MakePoint(\"LON\", \"LAT\"), 4326);\n",
    "\"\"\"\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "from pathlib import Path\n",
    "import shapefile\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pg_host = os.getenv('PG_HOST')\n",
    "pg_port = int(os.getenv('PG_PORT'))\n",
    "pg_user = os.getenv('PG_USER')\n",
    "pg_pass = os.getenv('PG_PASS')\n",
    "pg_db = os.getenv('PG_DB')\n",
    "\n",
    "pg_ibtracs_historical_table = os.getenv('PG_IBTRACS_HISTORICAL_TABLE')\n",
    "pg_ibtracs_active_table = os.getenv('PG_IBTRACS_ACTIVE_TABLE')\n",
    "pg_eccc_table = os.getenv('PG_ECCC_ACTIVE_TABLE')\n",
    "\n",
    "ibtracs_active_file = Path(os.getenv('ACTIVE_STORMS_CSV'))\n",
    "ibtracs_historical_file = Path(os.getenv('HISTORICAL_STORMS_CSV'))\n",
    "\n",
    "ibtracs_active_schema = Path(os.getenv('IBTRACS_ACTIVE_SCHEMA'))\n",
    "ibtracs_historical_schema = Path(os.getenv('IBTRACS_HISTORICAL_SCHEMA'))\n",
    "\n",
    "eccc_shp_path_src = Path(os.getenv('ECCC_SHP_SOURCE'))\n",
    "\n",
    "eccc_pts_schema = Path(os.getenv('ECCC_PTS_SCHEMA'))\n",
    "eccc_lin_schema = Path(os.getenv('ECCC_LIN_SCHEMA'))\n",
    "eccc_rad_schema = Path(os.getenv('ECCC_RAD_SCHEMA'))\n",
    "eccc_err_schema = Path(os.getenv('ECCC_ERR_SCHEMA'))\n",
    "\n",
    "process_data = os.getenv('PROCESS_DATA').split(\",\")\n",
    "\n",
    "# Tells pandas to skip the 2nd line in the CSV file that specifies unit types \n",
    "# for the columns - this row doesn't need to be inserted into the postgis table\n",
    "skip_rows = [1]\n",
    "\n",
    "# Empty values in the CSV file are represented by an empty string, this value \n",
    "# isn't accounted for by default in pandas, so it needs to be specified here, \n",
    "# a dictionary of values is also possible if there are other values that \n",
    "# should be interpreted as null or n/a\n",
    "na_values = ' '\n",
    "\n",
    "# IBTRACS data types, derived from postgres table structure\n",
    "table_dtypes = {\n",
    "    \"SID\": 'string', \n",
    "    \"SEASON\": 'Float32', \n",
    "    \"NUMBER\": 'Float32', \n",
    "    \"BASIN\": 'string', \n",
    "    \"SUBBASIN\": 'string', \n",
    "    \"NAME\": 'string', \n",
    "    \"ISO_TIME\": 'string', \n",
    "    \"NATURE\": 'string', \n",
    "    \"LAT\": 'Float32', \n",
    "    \"LON\": 'Float32', \n",
    "    \"WMO_WIND\": 'Float32', \n",
    "    \"WMO_PRES\": 'Float32', \n",
    "    \"WMO_AGENCY\": 'string', \n",
    "    \"TRACK_TYPE\": 'string', \n",
    "    \"DIST2LAND\": 'Float32', \n",
    "    \"LANDFALL\": 'Float32', \n",
    "    \"IFLAG\": 'string', \n",
    "    \"USA_AGENCY\": 'string', \n",
    "    \"USA_ATCF_ID\": 'string', \n",
    "    \"USA_LAT\": 'Float32', \n",
    "    \"USA_LON\": 'Float32', \n",
    "    \"USA_RECORD\": 'string', \n",
    "    \"USA_STATUS\": 'string', \n",
    "    \"USA_WIND\": 'Float32', \n",
    "    \"USA_PRES\": 'Float32', \n",
    "    \"USA_SSHS\": 'Float32', \n",
    "    \"USA_R34_NE\": 'Float32', \n",
    "    \"USA_R34_SE\": 'Float32', \n",
    "    \"USA_R34_SW\": 'Float32', \n",
    "    \"USA_R34_NW\": 'Float32', \n",
    "    \"USA_R50_NE\": 'Float32', \n",
    "    \"USA_R50_SE\": 'Float32', \n",
    "    \"USA_R50_SW\": 'Float32', \n",
    "    \"USA_R50_NW\": 'Float32', \n",
    "    \"USA_R64_NE\": 'Float32', \n",
    "    \"USA_R64_SE\": 'Float32', \n",
    "    \"USA_R64_SW\": 'Float32', \n",
    "    \"USA_R64_NW\": 'Float32', \n",
    "    \"USA_POCI\": 'Float32', \n",
    "    \"USA_ROCI\": 'Float32', \n",
    "    \"USA_RMW\": 'Float32', \n",
    "    \"USA_EYE\": 'Float32', \n",
    "    \"TOKYO_LAT\": 'Float32', \n",
    "    \"TOKYO_LON\": 'Float32', \n",
    "    \"TOKYO_GRADE\": 'Float32', \n",
    "    \"TOKYO_WIND\": 'Float32', \n",
    "    \"TOKYO_PRES\": 'Float32', \n",
    "    \"TOKYO_R50_DIR\": 'Float32', \n",
    "    \"TOKYO_R50_LONG\": 'Float32', \n",
    "    \"TOKYO_R50_SHORT\": 'Float32', \n",
    "    \"TOKYO_R30_DIR\": 'Float32', \n",
    "    \"TOKYO_R30_LONG\": 'Float32', \n",
    "    \"TOKYO_R30_SHORT\": 'Float32', \n",
    "    \"TOKYO_LAND\": 'Float32', \n",
    "    \"CMA_LAT\": 'Float32', \n",
    "    \"CMA_LON\": 'Float32', \n",
    "    \"CMA_CAT\": 'Float32', \n",
    "    \"CMA_WIND\": 'Float32', \n",
    "    \"CMA_PRES\": 'Float32', \n",
    "    \"HKO_LAT\": 'Float32', \n",
    "    \"HKO_LON\": 'Float32', \n",
    "    \"HKO_CAT\": 'string', \n",
    "    \"HKO_WIND\": 'Float32', \n",
    "    \"HKO_PRES\": 'Float32', \n",
    "    \"NEWDELHI_LAT\": 'Float32', \n",
    "    \"NEWDELHI_LON\": 'Float32', \n",
    "    \"NEWDELHI_GRADE\": 'string', \n",
    "    \"NEWDELHI_WIND\": 'Float32', \n",
    "    \"NEWDELHI_PRES\": 'Float32', \n",
    "    \"NEWDELHI_CI\": 'Float32', \n",
    "    \"NEWDELHI_DP\": 'Float32', \n",
    "    \"NEWDELHI_POCI\": 'Float32', \n",
    "    \"REUNION_LAT\": 'Float32', \n",
    "    \"REUNION_LON\": 'Float32', \n",
    "    \"REUNION_TYPE\": 'string', \n",
    "    \"REUNION_WIND\": 'Float32', \n",
    "    \"REUNION_PRES\": 'Float32', \n",
    "    \"REUNION_TNUM\": 'string', \n",
    "    \"REUNION_CI\": 'string', \n",
    "    \"REUNION_RMW\": 'Float32', \n",
    "    \"REUNION_R34_NE\": 'Float32', \n",
    "    \"REUNION_R34_SE\": 'Float32', \n",
    "    \"REUNION_R34_SW\": 'Float32', \n",
    "    \"REUNION_R34_NW\": 'Float32', \n",
    "    \"REUNION_R50_NE\": 'Float32', \n",
    "    \"REUNION_R50_SE\": 'Float32', \n",
    "    \"REUNION_R50_SW\": 'Float32', \n",
    "    \"REUNION_R50_NW\": 'Float32', \n",
    "    \"REUNION_R64_NE\": 'Float32', \n",
    "    \"REUNION_R64_SE\": 'Float32', \n",
    "    \"REUNION_R64_SW\": 'Float32', \n",
    "    \"REUNION_R64_NW\": 'Float32', \n",
    "    \"BOM_LAT\": 'Float32', \n",
    "    \"BOM_LON\": 'Float32', \n",
    "    \"BOM_TYPE\": 'string', \n",
    "    \"BOM_WIND\": 'Float32', \n",
    "    \"BOM_PRES\": 'Float32', \n",
    "    \"BOM_TNUM\": 'string', \n",
    "    \"BOM_CI\": 'string', \n",
    "    \"BOM_RMW\": 'Float32', \n",
    "    \"BOM_R34_NE\": 'Float32', \n",
    "    \"BOM_R34_SE\": 'Float32', \n",
    "    \"BOM_R34_SW\": 'Float32', \n",
    "    \"BOM_R34_NW\": 'Float32', \n",
    "    \"BOM_R50_NE\": 'Float32', \n",
    "    \"BOM_R50_SE\": 'Float32', \n",
    "    \"BOM_R50_SW\": 'Float32', \n",
    "    \"BOM_R50_NW\": 'Float32', \n",
    "    \"BOM_R64_NE\": 'Float32', \n",
    "    \"BOM_R64_SE\": 'Float32', \n",
    "    \"BOM_R64_SW\": 'Float32', \n",
    "    \"BOM_R64_NW\": 'Float32', \n",
    "    \"BOM_ROCI\": 'Float32', \n",
    "    \"BOM_POCI\": 'Float32', \n",
    "    \"BOM_EYE\": 'Float32', \n",
    "    \"BOM_POS_METHOD\": 'string', \n",
    "    \"BOM_PRES_METHOD\": 'string', \n",
    "    \"NADI_LAT\": 'Float32', \n",
    "    \"NADI_LON\": 'Float32', \n",
    "    \"NADI_CAT\": 'Float32', \n",
    "    \"NADI_WIND\": 'Float32', \n",
    "    \"NADI_PRES\": 'Float32', \n",
    "    \"WELLINGTON_LAT\": 'Float32', \n",
    "    \"WELLINGTON_LON\": 'Float32', \n",
    "    \"WELLINGTON_WIND\": 'Float32', \n",
    "    \"WELLINGTON_PRES\": 'Float32', \n",
    "    \"DS824_LAT\": 'Float32', \n",
    "    \"DS824_LON\": 'Float32', \n",
    "    \"DS824_STAGE\": 'string', \n",
    "    \"DS824_WIND\": 'Float32', \n",
    "    \"DS824_PRES\": 'Float32', \n",
    "    \"TD9636_LAT\": 'Float32', \n",
    "    \"TD9636_LON\": 'Float32', \n",
    "    \"TD9636_STAGE\": 'Float32', \n",
    "    \"TD9636_WIND\": 'Float32', \n",
    "    \"TD9636_PRES\": 'Float32', \n",
    "    \"TD9635_LAT\": 'Float32', \n",
    "    \"TD9635_LON\": 'Float32', \n",
    "    \"TD9635_WIND\": 'Float32', \n",
    "    \"TD9635_PRES\": 'Float32', \n",
    "    \"TD9635_ROCI\": 'Float32', \n",
    "    \"NEUMANN_LAT\": 'Float32', \n",
    "    \"NEUMANN_LON\": 'Float32', \n",
    "    \"NEUMANN_CLASS\": 'string', \n",
    "    \"NEUMANN_WIND\": 'Float32', \n",
    "    \"NEUMANN_PRES\": 'Float32', \n",
    "    \"MLC_LAT\": 'Float32', \n",
    "    \"MLC_LON\": 'Float32', \n",
    "    \"MLC_CLASS\": 'string', \n",
    "    \"MLC_WIND\": 'Float32', \n",
    "    \"MLC_PRES\": 'Float32', \n",
    "    \"USA_GUST\": 'Float32', \n",
    "    \"BOM_GUST\": 'Float32', \n",
    "    \"BOM_GUST_PER\": 'Float32', \n",
    "    \"REUNION_GUST\": 'Float32', \n",
    "    \"REUNION_GUST_PER\": 'Float32', \n",
    "    \"USA_SEAHGT\": 'Float32', \n",
    "    \"USA_SEARAD_NE\": 'Float32', \n",
    "    \"USA_SEARAD_SE\": 'Float32', \n",
    "    \"USA_SEARAD_SW\": 'Float32', \n",
    "    \"USA_SEARAD_NW\": 'Float32', \n",
    "    \"STORM_SPEED\": 'Float32', \n",
    "    \"STORM_DIR\": 'Float32'\n",
    "}\n",
    "\n",
    "# eccc_source_path = Path(os.getenv('ECCC_SHP_SOURCE'))\n",
    "\n",
    "engine = create_engine(f\"postgresql+psycopg2://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ibtracs(source_csv_file, destination_table, pg_engine, table_schema):\n",
    "    df = pd.read_csv(filepath_or_buffer=source_csv_file, header=0, skiprows=skip_rows, parse_dates=True, dtype=table_dtypes, na_values=na_values, keep_default_na=False)\n",
    "    \n",
    "    with pg_engine.begin() as pg_conn:\n",
    "        # Create Tables (if not exist using schemas)\n",
    "        # Generate shapes for IBTRACS?\n",
    "        print(\"Creating Table (if not exists)...\")\n",
    "        sql = Path(table_schema).read_text()\n",
    "        pg_conn.execute(text(sql))\n",
    "\n",
    "        # add lat/long geometry points\n",
    "        print(\"Adding Geometry Column...\")\n",
    "        sql = f'ALTER TABLE public.{destination_table} ADD COLUMN IF NOT EXISTS geom geometry(Point, 4326);'\n",
    "        pg_conn.execute(text(sql))\n",
    "        \n",
    "        # truncate tables\n",
    "        print(\"Clearing Existing Data...\")\n",
    "        sql = f\"DELETE FROM {destination_table};\"\n",
    "        pg_conn.execute(text(sql))\n",
    "\n",
    "        print(\"Committing Transaction.\")\n",
    "        pg_conn.execute(text(\"COMMIT;\"))\n",
    "    \n",
    "    # populate table\n",
    "    print(\"Populating Table...\")\n",
    "    df.to_sql(destination_table, pg_engine, chunksize=1000, method='multi', if_exists='append', index=False, schema='public')\n",
    "    \n",
    "    with pg_engine.begin() as pg_conn:\n",
    "        print(\"Updating Geometry...\")\n",
    "        sql = f'UPDATE public.{destination_table} SET geom = ST_SetSRID(ST_MakePoint(\"LON\", \"LAT\"), 4326);'\n",
    "        pg_conn.execute(text(sql))\n",
    "\n",
    "        print(\"Committing Transaction.\")\n",
    "        pg_conn.execute(text(\"COMMIT;\"))\n",
    "        print(\"Fin.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shp_to_json(shp_file:Path):\n",
    "    shp_file_obj = shapefile.Reader(shp_file.as_posix())\n",
    "\n",
    "    json_data = shp_file_obj.__geo_interface__\n",
    "\n",
    "    return(json_data)\n",
    "\n",
    "def populate_eccc_table(source_df, destination_table, pg_engine, table_schema):\n",
    "    with pg_engine.begin() as pg_conn:\n",
    "        # truncate tables\n",
    "        print(\"Clearing Existing Data...\")\n",
    "        sql = f\"DELETE FROM public.{destination_table};\"\n",
    "        pg_conn.execute(text(sql))\n",
    "\n",
    "        print(\"Committing Transaction.\")\n",
    "        pg_conn.execute(text(\"COMMIT;\"))\n",
    "\n",
    "    # populate table\n",
    "    print(\"Populating Table...\")\n",
    "    source_df.to_sql(destination_table, pg_engine, chunksize=1000, method='multi', if_exists='append', index=False, schema='public')\n",
    "\n",
    "def create_table_from_schema(pg_engine, table_name, schema_file, pg_schema='public'):\n",
    "    # Create ECCC Tables if not exist\n",
    "    with pg_engine.begin() as pg_conn:\n",
    "        print(f\"Creating Table {table_name} (if not exists)...\")\n",
    "\n",
    "        sql = f\"SELECT EXISTS (SELECT FROM pg_tables WHERE schemaname = '{pg_schema}' AND tablename = '{table_name}');\"\n",
    "        result = pg_conn.execute(text(sql))\n",
    "        table_exists = result.first()[0]\n",
    "\n",
    "        if not table_exists:\n",
    "            sql = Path(schema_file).read_text()\n",
    "            pg_conn.execute(text(sql))\n",
    "\n",
    "        print(\"Committing Transaction.\")\n",
    "        pg_conn.execute(text(\"COMMIT;\"))\n",
    "\n",
    "def build_eccc_points(json_data, df, pg_engine):\n",
    "    # return properties of features collection to form an array of \n",
    "    # point data\n",
    "    point_data = list(map(lambda feature: feature[\"properties\"], json_data[\"features\"]))[0]\n",
    "    point_string = f\"ST_MakePoint({point_data['LON']}, {point_data['LAT']})\"\n",
    "\n",
    "    # Use PostGIS to generate geometry string\n",
    "    with pg_engine.connect() as pg_conn:\n",
    "        sql = f\"SELECT {point_string} as geom_string;\"\n",
    "        result = pg_conn.execute(text(sql))\n",
    "        geo_string = result.first()[0]\n",
    "        point_data[\"geom\"] = geo_string\n",
    "\n",
    "    temp_df = pd.DataFrame([point_data])\n",
    "    \n",
    "    return pd.concat([df, temp_df])\n",
    "\n",
    "\n",
    "def build_eccc_lines(json_data, df, pg_engine, storm_date, storm_time):\n",
    "    line_data = json_data[\"features\"][0][\"properties\"]\n",
    "    \n",
    "    # Assemble timestamp to become part of table data\n",
    "    line_data[\"TIMESTAMP\"] = pd.to_datetime(f\"{storm_date} {storm_time}\", format=\"%Y%m%d %H%M%S\", utc=True)\n",
    "    \n",
    "    line_coords = list(map(lambda coord: f\"{coord[0]}, {coord[1]}\", json_data[\"features\"][0][\"geometry\"][\"coordinates\"]))\n",
    "\n",
    "    # Translate list of coordinates (2d Array LON/LAT) into a query for PostGIS\n",
    "    # to generate the geometry value for the dataframe\n",
    "    line_string = \"ST_Multi(ST_MakeLine(ARRAY[ST_MakePoint(\" + \"), ST_MakePoint(\".join(line_coords) + \")]))\"\n",
    "\n",
    "    # Use PostGIS to generate geometry string\n",
    "    with pg_engine.connect() as pg_conn:\n",
    "        sql = f\"SELECT {line_string} as geom_string;\"\n",
    "        result = pg_conn.execute(text(sql))\n",
    "        geo_string = result.first()[0]\n",
    "        line_data[\"geom\"] = geo_string\n",
    "\n",
    "    temp_df = pd.DataFrame([line_data])\n",
    "    \n",
    "    return pd.concat([df, temp_df])\n",
    "\n",
    "\n",
    "def build_eccc_wind_radii(json_data, df, pg_engine):\n",
    "    rad_data = list(map(lambda feature: feature[\"properties\"], json_data[\"features\"]))[0]\n",
    "    poly_coords = list(map(lambda coord: f\"{coord[0]}, {coord[1]}\", json_data[\"features\"][0][\"geometry\"][\"coordinates\"][0]))\n",
    "\n",
    "    # Translate list of coordinates (2d Array LON/LAT) into a query for PostGIS\n",
    "    # to generate the geometry value for the dataframe\n",
    "    poly_string = \"ST_Multi(ST_MakePolygon(ST_MakeLine(ARRAY[ST_MakePoint(\" + \"), ST_MakePoint(\".join(poly_coords) + \")])))\"\n",
    "\n",
    "    # Use PostGIS to generate geometry string\n",
    "    with pg_engine.connect() as pg_conn:\n",
    "        sql = f\"SELECT {poly_string} as geom_string;\"\n",
    "        result = pg_conn.execute(text(sql))\n",
    "        geo_string = result.first()[0]\n",
    "        rad_data[\"geom\"] = geo_string\n",
    "\n",
    "\n",
    "    temp_df = pd.DataFrame([rad_data])\n",
    "\n",
    "    return pd.concat([df, temp_df])\n",
    "\n",
    "def build_eccc_error_cone(json_data, df, pg_engine, storm_date, storm_time):\n",
    "    err_data = json_data[\"features\"][0][\"properties\"]\n",
    "    \n",
    "    # Assemble timestamp to become part of table data\n",
    "    err_data[\"TIMESTAMP\"] = pd.to_datetime(f\"{storm_date} {storm_time}\", format=\"%Y%m%d %H%M%S\", utc=True)\n",
    "    \n",
    "    line_coords = list(map(lambda coord: f\"{coord[0]}, {coord[1]}\", json_data[\"features\"][0][\"geometry\"][\"coordinates\"][0]))\n",
    "\n",
    "    # Translate list of coordinates (2d Array LON/LAT) into a query for PostGIS\n",
    "    # to generate the geometry value for the dataframe\n",
    "    line_string = \"ST_Multi(ST_MakePolygon(ST_MakeLine(ARRAY[ST_MakePoint(\" + \"), ST_MakePoint(\".join(line_coords) + \")])))\"\n",
    "\n",
    "    # Use PostGIS to generate geometry string\n",
    "    with pg_engine.connect() as pg_conn:\n",
    "        sql = f\"SELECT {line_string} as geom_string;\"\n",
    "        result = pg_conn.execute(text(sql))\n",
    "        geo_string = result.first()[0]\n",
    "        err_data[\"geom\"] = geo_string\n",
    "\n",
    "    temp_df = pd.DataFrame([err_data])\n",
    "    \n",
    "    return pd.concat([df, temp_df])\n",
    "\n",
    "def process_eccc_shp_files(source_dir, pg_engine):\n",
    "    source_search_pattern = \"**/*.shp\"\n",
    "    filename_parser = re.compile(r'(?P<date>\\d+)_(?P<time>\\d+)Z_(?P<storm>\\w+)\\.(?P<type>\\w+).*')\n",
    "\n",
    "    points_df = pd.DataFrame()\n",
    "    lines_df = pd.DataFrame()\n",
    "    wind_radii_df = pd.DataFrame()\n",
    "    error_cone_df = pd.DataFrame()\n",
    "\n",
    "    # Check and, if necessary, create ECCC tables\n",
    "    create_table_from_schema(pg_engine=pg_engine, table_name='eccc_storm_points', schema_file=eccc_pts_schema)\n",
    "    create_table_from_schema(pg_engine=pg_engine, table_name='eccc_storm_lines', schema_file=eccc_lin_schema)\n",
    "    create_table_from_schema(pg_engine=pg_engine, table_name='eccc_storm_wind_radii', schema_file=eccc_rad_schema)\n",
    "    create_table_from_schema(pg_engine=pg_engine, table_name='eccc_storm_error_cones', schema_file=eccc_err_schema)\n",
    "\n",
    "    for shp_file_path in Path(source_dir).glob(source_search_pattern):\n",
    "        shp_file = Path(shp_file_path)\n",
    "\n",
    "        (storm_date, storm_time, storm_name, data_type) = filename_parser.match(shp_file.name).groupdict().values()\n",
    "\n",
    "        json_data = shp_to_json(shp_file=shp_file)\n",
    "\n",
    "        if data_type == \"pts\":\n",
    "            points_df = build_eccc_points(json_data, points_df, pg_engine)\n",
    "\n",
    "        elif data_type == \"lin\":\n",
    "            lines_df = build_eccc_lines(json_data, lines_df, pg_engine, storm_date, storm_time)\n",
    "\n",
    "        elif data_type == \"rad\":\n",
    "            wind_radii_df = build_eccc_wind_radii(json_data, wind_radii_df, pg_engine)\n",
    "\n",
    "        elif data_type == \"err\":\n",
    "            error_cone_df = build_eccc_error_cone(json_data, error_cone_df, pg_engine, storm_date, storm_time)\n",
    "\n",
    "    populate_eccc_table(source_df=points_df, destination_table=\"eccc_storm_points\", pg_engine=pg_engine, table_schema=eccc_pts_schema)\n",
    "    populate_eccc_table(source_df=lines_df, destination_table=\"eccc_storm_lines\", pg_engine=pg_engine, table_schema=eccc_lin_schema)\n",
    "    populate_eccc_table(source_df=wind_radii_df, destination_table=\"eccc_storm_wind_radii\", pg_engine=pg_engine, table_schema=eccc_rad_schema)\n",
    "    populate_eccc_table(source_df=error_cone_df, destination_table=\"eccc_storm_error_cones\", pg_engine=pg_engine, table_schema=eccc_err_schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"IBTRACS\" in process_data:\n",
    "    print(\"Processing Active Storms...\")\n",
    "    process_ibtracs(source_csv_file=ibtracs_active_file, destination_table=pg_ibtracs_active_table, pg_engine=engine, table_schema=ibtracs_active_schema)\n",
    "\n",
    "    print(\"Processing Historical Storms...\")\n",
    "    process_ibtracs(source_csv_file=ibtracs_historical_file, destination_table=pg_ibtracs_historical_table, pg_engine=engine, table_schema=ibtracs_historical_schema)\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping IBTRACS data...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ECCC\" in process_data:\n",
    "    print(\"Processing ECCC shapefiles...\")\n",
    "    process_eccc_shp_files(eccc_shp_path_src, engine)\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping ECCC data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"End.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
