{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searches an ERDDAP Server based on a time range and then filters those \n",
    "# datasets by a list of standard names to find the appropriate variable names.\n",
    "\n",
    "import requests\n",
    "import httpx\n",
    "import pandas as pd\n",
    "from erddapy import ERDDAP\n",
    "\n",
    "server = \"https://cioosatlantic.ca/erddap\"\n",
    "e = ERDDAP(server=server)\n",
    "\n",
    "# Extracts data from the erddap metadata Pandas dataframe, NC_GLOBAL and\n",
    "# row type attribute are assumed as defaults for variable specific values\n",
    "# you'll need to specify those features\n",
    "def erddap_meta(metadata, attribute_name, row_type=\"attribute\", var_name=\"NC_GLOBAL\"):\n",
    "    # Example: uuid = metadata[(metadata['Variable Name']=='NC_GLOBAL') & (metadata['Attribute Name']=='uuid')]['Value'].values[0]\n",
    "    return_value = {\"value\": None, \"type\": None}\n",
    "\n",
    "    try:\n",
    "        return_value[\"value\"] = metadata[(metadata[\"Variable Name\"] == var_name) & (metadata[\"Attribute Name\"] == attribute_name)][\"Value\"].values[0]\n",
    "        return_value[\"type\"] = metadata[(metadata[\"Variable Name\"] == var_name) & (metadata[\"Attribute Name\"] == attribute_name)][\"Data Type\"].values[0]\n",
    "\n",
    "    except IndexError:\n",
    "        message = (\n",
    "            f\"IndexError (Not found?) extracting ERDDAP Metadata: attribute: {attribute_name}, row_type: {row_type}, var_name: {var_name}\"\n",
    "        )\n",
    "        print(message)\n",
    "\n",
    "    return return_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of standard names to query ERDDAP with because actual variable \n",
    "# names will vary from dataset to dataset.\n",
    "# \n",
    "# Querying by standard names gives us a universal method to interrogate a \n",
    "# dataset, resulting variable names can then be used to lookup long names and other attributes\n",
    "\n",
    "standard_names = [\n",
    "    \"wind_speed\",\n",
    "    \"wind_speed_of_gust\",\n",
    "    \"wind_from_direction\",\n",
    "    \"air_temperature\",\n",
    "    \"air_pressure\",\n",
    "    \"relative_humidity\",\n",
    "    \"sea_surface_temperature\",\n",
    "    \"sea_surface_wave_significant_height\",\n",
    "    \"sea_surface_wave_maximum_height\",\n",
    "    \"sea_surface_wave_maximum_period\",\n",
    "    \"sea_surface_wave_from_direction\",\n",
    "    \"sea_surface_wave_mean_period\",\n",
    "    \"sea_surface_wave_zero_upcrossing_period\"\n",
    "]\n",
    "\n",
    "# Date range for Hurricane FIONA (2022)\n",
    "min_time = \"2022-09-20\"\n",
    "max_time = \"2022-09-30\"\n",
    "\n",
    "# Get dataset list based on date range and other attributes if desired \n",
    "# (like a bounding box)\n",
    "search_url = e.get_search_url(response=\"csv\", min_time=min_time, max_time=max_time)\n",
    "search = pd.read_csv(search_url)\n",
    "\n",
    "dataset_list = search[\"Dataset ID\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interrogate each dataset for the list of variable names using the list \n",
    "# of standard names above\n",
    "\n",
    "final_dataset_list = {}\n",
    "\n",
    "for dataset_id in dataset_list:\n",
    "    dataset_vars = e.get_var_by_attr(dataset_id, standard_name=lambda std_name: std_name in standard_names)\n",
    "    \n",
    "    if dataset_vars:\n",
    "        # Fetch dataset metadata from ERDDAP based on dataset ID, assign to \n",
    "        # dictionary with variables of interest.\n",
    "\n",
    "        metadata_url = e.get_download_url(\n",
    "            dataset_id=f\"{dataset_id}/index\", response=\"csv\", protocol=\"info\"\n",
    "        )\n",
    "\n",
    "        metadata = pd.read_csv(filepath_or_buffer=metadata_url)\n",
    "        \n",
    "        final_dataset_list[dataset_id] = {\n",
    "            \"vars\" : [\"time\", \"latitude\", \"longitude\"] + dataset_vars,\n",
    "            \"meta\" : metadata\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        print(dataset_id, \"Doesn't have any matching variables.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through datasets and create a mapping between variable names and standard names\n",
    "for dataset_id in final_dataset_list.keys():\n",
    "    print(dataset_id)\n",
    "    \n",
    "    # A dictionary to hold the variable name mappings\n",
    "    replace_cols = {}\n",
    "\n",
    "    for var in final_dataset_list[dataset_id][\"vars\"]:\n",
    "        metadata = final_dataset_list[dataset_id][\"meta\"]\n",
    "\n",
    "        standard_name = erddap_meta(metadata=metadata, attribute_name=\"standard_name\", var_name=var)[\"value\"]\n",
    "        units = erddap_meta(metadata=metadata, attribute_name=\"units\", var_name=var)[\"value\"]\n",
    "        long_name = erddap_meta(metadata=metadata, attribute_name=\"long_name\", var_name=var)[\"value\"]\n",
    "\n",
    "        # Time columns usually have the unit of time in unix timestamp\n",
    "        if units.find(\"seconds since\") > -1:\n",
    "            units = \"UTC\"\n",
    "\n",
    "        # standard_name = metadata[(metadata[\"Variable Name\"] == var) & (metadata[\"Attribute Name\"] == \"standard_name\")][\"Value\"].values[0]\n",
    "        replace_cols[var] = f\"{standard_name}|{units}|{long_name}\"\n",
    "        print(var, \" => \", standard_name)\n",
    "    \n",
    "    print(replace_cols)\n",
    "\n",
    "    # Once variable names have been \n",
    "    e.protocol = \"tabledap\"\n",
    "    e.dataset_id = dataset_id\n",
    "    e.variables = final_dataset_list[dataset_id][\"vars\"]\n",
    "    e.constraints = {\n",
    "        \"time>=\": min_time,\n",
    "        \"time<=\": max_time\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        df = e.to_pandas()\n",
    "        # print(df.info())\n",
    "        \n",
    "        # !!! Uncomment this block to move time to the dataframe index and remove the original column !!!\n",
    "        #\n",
    "        # df[\"time (UTC)\"] = pd.to_datetime(df[\"time (UTC)\"])\n",
    "        # df.set_index(df['time (UTC)'], inplace=True)\n",
    "        # df.drop(\"time (UTC)\", axis=\"columns\", inplace=True)\n",
    "        # del replace_cols['time']\n",
    "\n",
    "        # Remap columns to incorporate standard name, long name and units\n",
    "        df.columns = map(lambda col: col + \" (\" + replace_cols[col] + \")\", replace_cols.keys())\n",
    "        print(df.head(3))\n",
    "\n",
    "    except (requests.HTTPError, httpx.HTTPError) as ex:\n",
    "        print(\"HTTPError\", ex)\n",
    "        print(f\" - No data found for time range: {min_time} - {max_time}\")\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
